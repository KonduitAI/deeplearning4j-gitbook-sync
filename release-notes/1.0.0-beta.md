# 1.0.0-beta

## Highlights - 1.0.0-beta Release

* Performance and memory optimizations for DL4J

## Deeplearning4J

### Deeplearning4J: New Features

* New or enhanced layers:
  * Added Cropping1D layer [Link](https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/convolutional/Cropping1D.java)
  * Added Convolution3D, Cropping3D, UpSampling3D, ZeroPadding3D, Subsampling3D layers (all with Keras import support): [Link](https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/Convolution3D.java) [Link](https://github.com/eclipse/deeplearning4j/pull/5026)
  * Added EmbeddingSequenceLayer (EmbeddingLayer for time series) [Link](https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/EmbeddingSequenceLayer.java)
  * Added OCNNOutputLayer (one-class neural network) - implementation of [this paper](https://arxiv.org/pdf/1802.06360.pdf) - [Link](https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ocnn/OCNNOutputLayer.java)
  * Added FrozenLayerWithBackprop layer [Link](https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/misc/FrozenLayerWithBackprop.java)
  * Added DepthwiseConvolution2D layer [Link](https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/DepthwiseConvolution2D.java)
* Added ComputationGraph.output(DataSetIterator) method [Link](https://github.com/eclipse/deeplearning4j/issues/4965)
* Added MultiLayerNetwork/ComputationGraph.layerInputSize methods [Link](https://github.com/eclipse/deeplearning4j/issues/4670) [Link](https://github.com/eclipse/deeplearning4j/pull/5018)
* Added SparkComputationGraph.feedForwardWithKey overload with feature mask support [Link](https://github.com/eclipse/deeplearning4j/issues/4984)
* Added MultiLayerNetwork.calculateGradients method (for easily getting parameter and input gradients, for example for some model interpretabilithy approaches) [Link](https://github.com/eclipse/deeplearning4j/pull/5018) [Link](https://github.com/eclipse/deeplearning4j/issues/2866)
* Added support to get input/activation types for each layer from configuration: `ComputationGraphConfiguration.getLayerActivationTypes(InputType...)`, `ComputationGraphConfiguration.GraphBuilder.getLayerActivationTypes()`, `NeuralNetConfiguration.ListBuilder.getLayerActivationTypes()`, `MultiLayerConfiguration.getLayerActivationTypes(InputType)` methods [Link](https://github.com/eclipse/deeplearning4j/pull/5031)
* Evaluation.stats() now prints confusion matrix in easier to read matrix format, rather than list format [Link](https://github.com/eclipse/deeplearning4j/issues/5096)
* Added ModelSerializer.addObjectToFile, .getObjectFromFile and .listObjectsInFile for storing arbitrary Java objects in same file as saved network [Link](https://github.com/eclipse/deeplearning4j/issues/4957)
* Added SpatialDropout support (with Keras import support) [Link](https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/dropout/SpatialDropout.java)
* Added `MultiLayerNetwork/ComputationGraph.fit((Multi)DataSetIterator, int numEpochs)` overloads [Link](https://github.com/eclipse/deeplearning4j/pull/5118)
* Added performance (hardware) listeners: `SystemInfoPrintListener` and `SystemInfoFilePrintListener` [Link](https://github.com/eclipse/deeplearning4j/pull/5151)

### Deeplearning4J: Bug Fixes and Optimizations

* Performance and memory optimizations via optimizations of internal use of workspaces [Link](https://github.com/eclipse/deeplearning4j/pull/4900)
* Reflections library has entirely been removed from DL4J and is no longer required for custom layer serialization/deserialization [Link](https://github.com/eclipse/deeplearning4j/pull/4950), [Link](https://github.com/eclipse/deeplearning4j/pull/4956)
  * Fixes issues with custom and some Keras import layers on Android
* RecordReaderMultiDataSetIterator will no longer try to convert unused columns to numerical values [Link](https://github.com/eclipse/deeplearning4j/pull/4945)
* Added new model zoo models:
  * (to do)
* Fixes for Android compilation (removed duplicate classes, aligned versions, removed some dependencies) [Link](https://github.com/eclipse/deeplearning4j/pull/4955) [Link](https://github.com/eclipse/deeplearning4j/pull/5074) [Link](https://github.com/eclipse/deeplearning4j/issues/5087)
* Fix for RecordReaderMulitDataSetIterator where output could be incorrect for some constructors [Link](https://github.com/eclipse/deeplearning4j/issues/4969)
* Non-frozen layers before a frozen layer will no longer be skipped during backprop (useful for GANs and similar architectures) [Link](https://github.com/eclipse/deeplearning4j/pull/5009) [Link](https://github.com/eclipse/deeplearning4j/issues/4964)
* Fixed issue where ComputationGraph topological sort may not be consistent on all platforms; could sometimes break ComputationGraphs (with multiple valid topological orderings) trained on PC and deployed on Android [Link](https://github.com/eclipse/deeplearning4j/pull/5050)
* Fixed issue with CuDNN batch norm using `1-decay` instead of `decay` [Link](https://github.com/eclipse/deeplearning4j/pull/5076)
* deeplearning4j-cuda no longer throws exceptions if present on classpath with nd4j-native backend set to higher priority [Link](https://github.com/eclipse/deeplearning4j/issues/5000)
* Added RNG control for CifarDataSetIterator [Link](https://github.com/eclipse/deeplearning4j/issues/5067)
* WordVectorSerializer now deletes temp files immediately once done [Link](https://github.com/eclipse/deeplearning4j/pull/5166)

### Deeplearning4J: API Changes (Transition Guide): 1.0.0-alpha to 1.0.0-beta

* WorkspaceMode.SINGLE and SEPARATE have been deprecated; use WorkspaceMode.ENABLED instead
* Internal layer API changes: custom layers will need to be updated to the new Layer API - see built-in layers or custom layer example
* Custom layers etc in pre-1.0.0-beta JSON (ModelSerializer) format need to be registered before they can be deserialized due to JSON format change. Built-in layers and models saved in 1.0.0-beta or later do not require this. Use `NeuralNetConfiguration.registerLegacyCustomClassesForJSON(Class)` for this purpose
* IterationListener has been deprecated in favor of TrainingListener. For existing custom listeners, switch from `implements TrainingListener` to `extends BaseTrainingListener` [Link](https://github.com/eclipse/deeplearning4j/pull/5014)
* ExistingDataSetIterator has been deprecated; use `fit(DataSetIterator, int numEpochs)` method instead

### Deelpearning4J: 1.0.0-beta Known Issues

* ComputationGraph TrainingListener onEpochStart and onEpochEnd methods are not being called correctly
* DL4J Zoo Model FaceNetNN4Small2 model configuration is incorrect, causing issues during forward pass
* Early stopping score calculators with values thar should be maximized (accuracy, f1 etc) are not working properly (values are minimized not maximized). Workaround: override `ScoreCalculator.calculateScore(...)` and return `1.0 - super.calculateScore(...)`.

## Deeplearing4J: Keras Import

### Deeplearning4J: Keras Import - API Changes (Transition Guide): 1.0.0-alpha to 1.0.0-beta

## ND4J

### ND4J: New Features

### ND4J: Known Issues

* Not all op gradients implemented for automatic differentiation
* Vast majority of new operations added in 1.0.0-beta do NOT use GPU yet.

### ND4J: API Changes (Transition Guide): 1.0.0-alpha to 1.0.0-beta

## DataVec

### DataVec: New Features

* ImageRecordReader now logs number of inferred label classes (to reduce risk of users missing a problem if something is misconfigured) [Link](https://github.com/deeplearning4j/DataVec/issues/569)
* Added AnalyzeSpark.getUnique overload for multiple columns [Link](https://github.com/deeplearning4j/DataVec/issues/71)
* Added performance/timing module [Link](https://github.com/deeplearning4j/DataVec/pull/580)

### DataVec: Optimizations and Bug Fixes

* Reduced ImageRecordReader garbage generation via buffer reuse [Link](https://github.com/deeplearning4j/DataVec/pull/573)
* Fixes for Android compilation (aligned versions, removed some dependencies) [Link](https://github.com/deeplearning4j/DataVec/pull/567) [Link](https://github.com/deeplearning4j/DataVec/pull/575)
* Removed Reflections library use in DataVec [Link](https://github.com/deeplearning4j/DataVec/pull/570)
* Fix for TransformProcessRecordReader batch support [Link](https://github.com/deeplearning4j/DataVec/issues/561)
* Fix for TransformProcessRecordReader with filter operations [Link](https://github.com/deeplearning4j/DataVec/issues/552)
* Fixed issue with ImageRecordReader/ParentPathLabelGenerator incorrectly filtering directories containing `.` character(s) [Link](https://github.com/deeplearning4j/DataVec/issues/273)
* ShowImageTransform now initializes frame lazily to avoid blank windows [Link](https://github.com/deeplearning4j/DataVec/pull/579)

### DataVec: API Changes (Transition Guide): 1.0.0-alpha to 1.0.0-beta

* DataVec ClassPathResource has been deprecated; use nd4j-common version instead [Link](https://github.com/deeplearning4j/DataVec/issues/521)

## Arbiter

### Arbiter: New Features

* Added LayerSpace for OCNN (one-class neural network)

### Arbiter: Fixes

* Fixed timestamp issue that could cause incorrect rendering of first model's results in UI [Link](https://github.com/deeplearning4j/Arbiter/pull/163)
* Execution now waits for last model(s) to complete before returning when a termination condition is hit [Link](https://github.com/deeplearning4j/Arbiter/pull/162)
* As per DL4J etc: use of Reflections library has been removed entirely from Arbiter [Link](https://github.com/deeplearning4j/Arbiter/pull/154)
* Remove use of Eclipse Collections library due to issues with Android compilation [Link](https://github.com/deeplearning4j/Arbiter/pull/156)
* Improved cleanup of completed models to reduce maximum memory requirements for training [Link](https://github.com/deeplearning4j/Arbiter/pull/160)
